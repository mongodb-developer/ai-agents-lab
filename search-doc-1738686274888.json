{"searchDocs":[{"title":"üëê Add memory to the agent","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/adding-memory/adding-memory","content":"üëê Add memory to the agent The final step in this lab is to add conversational message history as a form of memory for the agent. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 11: Add memory to the agent section in the notebook to add memory to the agent. The answers for code blocks in this section are as follows: CODE_BLOCK_21 Answer {&quot;configurable&quot;: {&quot;thread_id&quot;: thread_id}} ","keywords":"","version":"Next"},{"title":"üìò Lecture notes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/adding-memory/lecture-notes","content":"","keywords":"","version":"Next"},{"title":"Checkpoints‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/adding-memory/lecture-notes#checkpoints","content":" Checkpoints in LangGraph are a snapshot of the graph state. This is how AI applications built using LangGraph persist short-term and long-term memory.  ","version":"Next","tagName":"h2"},{"title":"Thread IDs‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/adding-memory/lecture-notes#thread-ids","content":" Thread IDs are unique IDs assigned to memory checkpoints in LangGraph, allowing it to distinguish between conversation threads, facilitate human-in-the loop workflows and allow users to review and debug graph executions. ","version":"Next","tagName":"h2"},{"title":"üëê Create a vector search index","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/create-vector-search-index","content":"üëê Create a vector search index To retrieve documents using vector search, you must configure a vector search index on the collection you want to perform vector search against. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 3: Create a vector search index section in the notebook to create a vector search index. The answers for code blocks in this section are as follows: CODE_BLOCK_3 Answer vs_collection.create_search_index(model=model) To verify that the index was created, navigate to the Overview page in the Atlas UI. In the Clusters section, select your cluster and click Browse collections. Navigate to Search Indexes for the chunked_docs collection in the mongodb_genai_devday database. The index is ready to use once the status changes from PENDING to READY.","keywords":"","version":"Next"},{"title":"üëê Import data into MongoDB","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/import-data","content":"üëê Import data into MongoDB caution If you are doing this lab as part of a MongoDB GenAI Developer Day, at this point you should already have the data required for this lab in your MongoDB cluster, so you can skip this step. If you haven't imported the data yet, follow the steps below to do so. The documentation agent has two tools- a vector search tool to retrieve information from documentation to answer questions, and another tool to get the content from specific documentation pages for summarization. Let's import the data required by these tools into two MongoDB collections. This is as simple as making a POST request to a serverless function that we have created for you. Run the cells under the Step 2: Import data section in the notebook to import the data required by the agent's tools, into MongoDB collections. To verify that the data has been imported into your MongoDB cluster, navigate to the Overview page in the Atlas UI. In the Clusters section, select your cluster and click Browse collections. Ensure that you see a database called mongodb_genai_devday, and two collections namely chunked_docs and full_docs under it. Note the number and format of documents in both the collections.","keywords":"","version":"Next"},{"title":"üìò Lecture notes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/lecture-notes","content":"","keywords":"","version":"Next"},{"title":"About the data‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/agent-tools/lecture-notes#about-the-data","content":" In this lab, we are using a serverless function to import the data required by the agent's tools, into MongoDB. If you want to do this on your own, these datasets are available on Hugging Face:  mongodb-docs: Markdown versions of a small subset of MongoDB's technical documentation. This dataset is imported into a collection called full_docs. mongodb-docs-embedded: Chunked and embedded versions of the articles in the mongodb-docs dataset. This dataset is imported into a collection called chunked_docs.  To learn more about chunking and embedding, here are some resources from our Developer Center:  How to Choose the Right Chunking Strategy for Your LLM ApplicationHow to Choose the Best Embedding Model for Your LLM Application  ","version":"Next","tagName":"h2"},{"title":"Tool calling‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/agent-tools/lecture-notes#tool-calling","content":" Tool calling, interchangeably called function calling allows an LLM to use external tools such as APIs, databases, specialized machine learning models etc.  In AI agents, an LLM can have access to multiple tools. Given a user query, the LLM decides which tool to invoke and the arguments for the tool call. These arguments are used to execute the tool call and the output is returned back to the LLM to inform its next steps.  The easiest way to define tools in LangChain is using the @tool decorator. The decorator makes tools out of functions by using the function name as the tool name by default, and the function's docstring as the tool's description. The tool call inturn consists of a tool name, arguments, and an optional identifier.  An example of a tool in LangChain is as follows:  @tool(&quot;search-tool&quot;, return_direct=True) def search(query: str) -&gt; str: &quot;&quot;&quot;Look up things online.&quot;&quot;&quot; return &quot;MongoDB&quot;   An example of a tool call is as follows:  { &quot;name&quot;: &quot;search-tool&quot;, &quot;args&quot;: { &quot;query&quot;: &quot;What is MongoDB?&quot; }, &quot;id&quot;: &quot;call_H5TttXb423JfoulF1qVfPN3m&quot; }   ","version":"Next","tagName":"h2"},{"title":"Semantic Search in MongoDB‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/agent-tools/lecture-notes#semantic-search-in-mongodb","content":" You can learn more about semantic search in MongoDB here. ","version":"Next","tagName":"h2"},{"title":"üëê Create agent tools","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/agent-tools/create-agent-tools","content":"üëê Create agent tools The easiest way to define custom tools for agents in LangChain is using the @tool decorator. The decorator makes tools out of functions by using the function name as the tool name by default, and the function's docstring as the tool's description. We want the documentation agent to have access to the following tools: get_information_for_question_answering: Uses vector search to retrieve information to answer questions get_page_content_for_summarization: Gets the content of specific document pages for summarization Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 4: Create agent tools section in the notebook to create tools for the agent to use. The answers for code blocks in this section are as follows: CODE_BLOCK_4 Answer embedding_model.encode(text) CODE_BLOCK_5 Answer get_embedding(user_query) CODE_BLOCK_6 Answer [ { &quot;$vectorSearch&quot;: { &quot;index&quot;: VS_INDEX_NAME, &quot;path&quot;: &quot;embedding&quot;, &quot;queryVector&quot;: query_embedding, &quot;numCandidates&quot;: 150, &quot;limit&quot;: 5, } }, { &quot;$project&quot;: { &quot;_id&quot;: 0, &quot;body&quot;: 1, &quot;score&quot;: {&quot;$meta&quot;: &quot;vectorSearchScore&quot;}, } }, ] CODE_BLOCK_7 Answer vs_collection.aggregate(pipeline) CODE_BLOCK_8 Answer {&quot;title&quot;: user_query} CODE_BLOCK_9 Answer {&quot;_id&quot;: 0, &quot;body&quot;: 1} CODE_BLOCK_10 Answer full_collection.find_one(query, projection) ","keywords":"","version":"Next"},{"title":"üëê Build and execute the graph","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/build-and-execute-graph","content":"üëê Build and execute the graph Now that we have defined the nodes and edges of the graph, let's put the graph together and execute it to ensure that the agent is working as expected. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 9: Build the graph and Step 10: Execute the graph sections in the notebook to build and execute the graph. The answers for code blocks in this section are as follows: CODE_BLOCK_16 Answer graph.add_node(&quot;agent&quot;, agent) CODE_BLOCK_17 Answer graph.add_node(&quot;tools&quot;, tool_node) CODE_BLOCK_18 Answer graph.add_edge(START, &quot;agent&quot;) CODE_BLOCK_19 Answer graph.add_edge(&quot;tools&quot;, &quot;agent&quot;) CODE_BLOCK_20 Answer graph.add_conditional_edges( &quot;agent&quot;, route_tools, {&quot;tools&quot;: &quot;tools&quot;, END: END}, ) caution Upon executing the graph, if you see the agent get stuck in an infinite tool-calling loop, play around with the prompt in Step 7 and run the cells that follow until you initialize the llm_with_tools variable. If this doesn't work, consider this a lesson in working with non-deterministic ML models. üôÇ","keywords":"","version":"Next"},{"title":"üëê Define graph state","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/define-graph-state","content":"üëê Define graph state Let's start by defining the state of the agent's graph. Run the cells under the Step 5: Define graph state section in the notebook to define the graph state for the AI agent.","keywords":"","version":"Next"},{"title":"üëê Define graph nodes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/define-graph-nodes","content":"üëê Define graph nodes Let's define the nodes of the graph. The agent will have two nodes- an agent node and a tool node. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 7: Define graph nodes section in the notebook to define the nodes of the graph. The answers for code blocks in this section are as follows: CODE_BLOCK_13 Answer state[&quot;messages&quot;] CODE_BLOCK_14 Answer llm_with_tools.invoke(messages) CODE_BLOCK_15 Answer tool.invoke(tool_call[&quot;args&quot;]) ","keywords":"","version":"Next"},{"title":"üëê Instantiate the LLM","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/instantiate-llm","content":"üëê Instantiate the LLM Now let's instantiate the LLM that will serve as the &quot;brain&quot; of the agent, and give it access to the tools we defined previously. Fill in any &lt;CODE_BLOCK_N&gt; placeholders and run the cells under the Step 6: Instantiate the LLM section in the notebook to initialize the LLM for the agent and give it access to tools. The answers for code blocks in this section are as follows: CODE_BLOCK_11 Answer llm.bind_tools(tools) CODE_BLOCK_12 Answer prompt | bind_tools ","keywords":"","version":"Next"},{"title":"üëê Define conditional edges","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/define-conditional-edges","content":"üëê Define conditional edges Edges in a LangGraph graph can be fixed or conditional. For conditional edges, we need a routing function to conditionally route the workflow to different nodes. Run the cells under the Step 8: Define conditional edges section in the notebook to define the routing function for the one conditional edge in the graph.","keywords":"","version":"Next"},{"title":"üìò Lecture notes","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/create-agent/lecture-notes","content":"","keywords":"","version":"Next"},{"title":"Creating agents using LangGraph‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#creating-agents-using-langgraph","content":" In this lab, we will use LangGraph by LangChain to orchestrate an AI agen for a technical documentation website. LangGraph allows you to model agentic systems as graphs. Graphs in LangGraph have the following core features:  ","version":"Next","tagName":"h2"},{"title":"State‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#state","content":" Each graph in has a state which is a shared data structure that all the nodes can access and make updates to. You can define custom attributes within the state depending on what parameters you want to track across the nodes of the graph.  ","version":"Next","tagName":"h3"},{"title":"Nodes‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#nodes","content":" Nodes in LangGraph are Python functions that encode the logic of your agents. They receive the current state of the graph as input, perform some computation and return an updated state.  ","version":"Next","tagName":"h3"},{"title":"Edges‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#edges","content":" Edges in LangGraph are Python functions that determine which graph node to execute next based on the current state of the graph. Edges can be conditional or fixed.  ","version":"Next","tagName":"h3"},{"title":"Using different LLM providers with LangChain‚Äã","type":1,"pageTitle":"üìò Lecture notes","url":"/ai-agents-lab/docs/create-agent/lecture-notes#using-different-llm-providers-with-langchain","content":" LangChain supports different LLM providers for you to build AI applications with. Unless you are using open-source models, you typically need to obtain API keys to use the chat completion APIs offered by different LLM providers.  For this lab, we have created a serverless function that creates LLM objects for Amazon, Google and Microsoft models that you can use with LangChain and LangGraph without having to obtain API keys. However, if you would like to do this on your own, here are some resources:  Using Amazon Bedrock LLMs with LangChain Using Google LLMs with LangChain Using Microsoft LLMs with langChain ","version":"Next","tagName":"h2"},{"title":"üëê MongoDB Setup","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/getting-started/mongodb-setup","content":"üëê MongoDB Setup caution If you are doing this lab as part of a MongoDB GenAI Developer Day, at this point you should already have a free cluster, so you can skip this step. If you don't have a cluster yet, follow the steps below to create one. In this lab, you will learn how to use MongoDB Atlas as a knowledge base as well as a memory provider to build an AI agent for a technical documentation website. To use MongoDB Atlas, you will need to create an account, a free cluster and obtain the connection string to connect to your cluster. Follow these steps to get set up: Register for a free MongoDB Atlas account Create a new database cluster Obtain the connection string for your database cluster","keywords":"","version":"Next"},{"title":"üëê Setup dev environment","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/getting-started/dev-env-setup","content":"","keywords":"","version":"Next"},{"title":"Option 1: GitHub Codespaces‚Äã","type":1,"pageTitle":"üëê Setup dev environment","url":"/ai-agents-lab/docs/getting-started/dev-env-setup#option-1-github-codespaces","content":" You will be working in a Jupyter Notebook in a GitHub Codespace throughout this lab. A codespace is a cloud-hosted, containerized development environment that comes pre-configured with all the tools you need to run this lab.  Navigate to this link. You will be prompted to sign into GitHub if you haven't already. Once signed in, click the Create new codespace button to create a new codespace.    Let it run for a few seconds as it prepares your environment. It will clone the repository, prepare the container, and run the installation scripts. Once the environment is built, you should see a list of files appear under the Explorer.  In the left navigation bar of the IDE, click on the file named ai-agents-lab.ipynb to open the Jupyter Notebook for this lab.    Next, select the Python interpreter by clicking Select Kernel at the top right of the IDE.    In the modal that appears, click Python environments... and select the interpreter that is marked as Recommended or Global Env.      That's it! You're ready for the lab!  ","version":"Next","tagName":"h2"},{"title":"Option 2: Run locally‚Äã","type":1,"pageTitle":"üëê Setup dev environment","url":"/ai-agents-lab/docs/getting-started/dev-env-setup#option-2-run-locally","content":" caution During the lab, we will use GitHub Codespaces. These instructions are here just in case you can't use Codespaces or if you really, really, really want a local installation.  If you want to run the notebook locally, follow the steps below:  Clone the GitHub repo for this lab by executing the following command from the terminal:  git clone https://github.com/mongodb-developer/genai-devday-notebooks.git   cd into the cloned directory:  cd genai-devday-notebooks   Create and activate a Python virtual environment:  python -m venv mongodb-ai-agents-lab source mongodb-ai-agents-lab/bin/activate   Install the dependencies for this lab:  pip install -r requirements.txt   Install and launch Jupyter Notebook:  pip install notebook jupyter notebook   In the browser tab that pops up, open the file named ai-agents-lab.ipynb.   ","version":"Next","tagName":"h2"},{"title":"üëê Setup prerequisites","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/getting-started/setup-pre-reqs","content":"üëê Setup prerequisites Fill in any &lt;CODE_BLOCK_N&gt; placeholders, select the LLM provider recommended by your instructor, and run the cells under the Step 1: Setup prerequisites section in the notebook. The answers for code blocks in this section are as follows: CODE_BLOCK_1 Answer mongodb_client[DB_NAME][VS_COLLECTION_NAME] CODE_BLOCK_2 Answer mongodb_client[DB_NAME][FULL_COLLECTION_NAME] ","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/intro","content":"Introduction Lab goals\tLearn the basics of building AI agentsWhat you'll learn\tWhat are AI agents When to use AI agents? Components of an AI agent Agent Architectures Building an AI agent Adding memory to agents Time to complete\t90 mins In the navigation bar and in some pages, you will notice some icons. Here is their meaning: Icon\tMeaning\tDescriptionüìò\tLecture material\tIf you are following along in an instructor-led session, they probably have covered this already. üëê\tHands-on content\tGet ready to do some hands-on work. You should follow these steps. üìö\tDocumentation\tReference documentation for hands-on portions of the lab. ü¶π\tAdvanced content\tThis content isn't covered during the lab, but if you're interested in learning more, you can check it out.","keywords":"","version":"Next"},{"title":"üìò What are AI agents?","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/what-are-ai-agents","content":"üìò What are AI agents? An AI agent is a system that uses an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools. In multi-agent systems, two or more agents collaborate or orchestrate and delegate tasks to solve problems. This way, agentic systems can handle complex, multi-step queries, and also self-revise and refine responses.","keywords":"","version":"Next"},{"title":"üìò Components of AI agents","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/components-of-agents","content":"","keywords":"","version":"Next"},{"title":"Perception‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#perception","content":" Perception, in the context of AI agents, is the mechanism by which the agent gathers information about its environment. Text inputs are currently the most common perception mechanism for AI agents, but we are slowly progressing towards audio, visual, multimodal or even physical sensory inputs.  ","version":"Next","tagName":"h2"},{"title":"Planning and reasoning‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#planning-and-reasoning","content":" AI agents use user prompts, self-prompting and feedback loops to break down complex tasks, reason through their execution plan and refine it as needed.  Some common design patterns for planning and reasoning in AI agents are as follows:  ","version":"Next","tagName":"h2"},{"title":"Chain of Thought (Cot) Prompting‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#chain-of-thought-cot-prompting","content":" In this approach, the LLM is prompted to generate a step-by-step explanation or reasoning process for a given task or problem.  Here is an example of a zero-shot CoT prompt:  Given a question, write out in a step-by-step manner your reasoning for how you will solve the problem to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.  ","version":"Next","tagName":"h3"},{"title":"ReAct (Reason + Act)‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#react-reason--act","content":" In this approach, the LLM is prompted to generate reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans, while actions allow it to interface with external sources or tools, to gather additional information.  Here is an example of a ReAct prompt:  Answer the following questions as best you can. You have access to the following tools:{tools} ## Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question   ","version":"Next","tagName":"h3"},{"title":"Reflection‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#reflection","content":" Reflection involves prompting an LLM to reflect on and critique past actions, sometimes incorporating additional external information such as tool observations. The generation-reflection loop is run several times before returning the final response to the user. Reflection trades a bit of extra compute for a shot at better output quality.  ","version":"Next","tagName":"h3"},{"title":"Tools‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#tools","content":" Tools are interfaces for AI agents to interact with the external world in order to achieve their objectives. These can be APIs, vector databases, or even specialized machine learning models.  ","version":"Next","tagName":"h2"},{"title":"Memory‚Äã","type":1,"pageTitle":"üìò Components of AI agents","url":"/ai-agents-lab/docs/key-concepts/components-of-agents#memory","content":" The memory component allows AI agents to store and recall past conversations, enabling them to learn from these interactions.  There are two main types of memory for AI agents:  Short-term memory: Stores and retrieves information from a specific conversation. Long-term memory: Stores, retrieves and updates information based on multiple conversations had over a period of time. ","version":"Next","tagName":"h2"},{"title":"üìò When to use AI agents?","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/key-concepts/when-to-use-agents","content":"üìò When to use AI agents? AI agents are best suited for complex, multi-step tasks that require integration of multiple capabilities, such as question-answering, analysis, task execution etc. to arrive at the final answer or outcome. An active area of research is to have AI agents learn from their past interactions to build personalized and adaptive experiences. Here are some examples of tasks/questions that DO NOT require an AI agent: Who was the first president of the United States? The information required to answer this question is very likely present in the parametric knowledge of most LLMs. Hence, this question can be answer using a simple prompt to an LLM. What is the reimbursement policy for meals for my company? The information required to answer this question is most likely not present in the parametric knowledge of available LLMs. However, this question can easily be answered using Retrieval Augmented Generation (RAG) using a knowledge base consisting of your company's data. This still does not require an agent. Here are some use cases for AI agents: How has the trend in the average daily calorie intake among adults changed over the last decade in the United States, and what impact might this have on obesity rates? Additionally, can you provide a graphical representation of the trend in obesity rates over this period? This question involves multiple sub-tasks such as data aggregation, visualization, and reasoning. Hence, this is a good use case for an AI agent. Creating a personalized learning assistant that can adjust its language, examples, and methods based on the student‚Äôs responses. This is an example of a complex task which also involves user personalization. Hence, this is a good fit for an AI agent.","keywords":"","version":"Next"},{"title":"üéØ Summary","type":0,"sectionRef":"#","url":"/ai-agents-lab/docs/summary","content":"üéØ Summary Congratulations! Following this lab, you have successfully: learned what are AI agentslearned when to use AI agentslearned about different agent architecturesbuilt an AI agent with memory Here are some resources that you might find helpful: MongoDB Developer CenterGenAI Code Examples RepositoryGenAI Community Forums","keywords":"","version":"Next"}],"options":{"id":"default"}}